## 文本分析

  - ES在索引或搜索text类型属性时进行文本分析，它会进行全文检索，搜索结果包含所有相关的结果，不只是完全匹配的结果
  - Tokenization:
    - 全文检索可以通过令牌化进行分析，将一个text划分为更小的块，成为tokens。通常这些token就是独立的单词
  - 标准化：
    - Tokenization启用对独立的项(term)进行匹配，但是每一项依然按照字面做匹配，例如：搜索Quick不会匹配到quick
    - 文本分析可以通过标准化将这些tokens转换为标准格式，因此可以匹配到相似格式，不是完全一致的内容
  - 文本分析通过分析器完成，它是一系列控制整个过程的规则，ES包含一个默认的分析器

## 分析器

  - 分析器：
    - 分析器主要包括3部分：character filters, tokenizers, and token filters
    - 字符过滤器：将接收到的原始文本作为字符流处理，可以添加，删除或更新字符
    - Tokenizer：接收字符流，将它划分为tokens，并输出tokens流
    - Token过滤器：接收token流，可以添加，删除或更新token
  - 索引和搜索分析：
    - 文本分析发生在2个时间：
      - 索引时间：当文档被索引时，任意text类型属性值会被分析
      - 搜索时间：当对text属性进行全文检索时，查询语句会被分析
    - 分析过程：
      - 一个文档包含如下text属性：
        ![image](https://user-images.githubusercontent.com/46510621/132093661-1da9ef6e-134d-43b3-88c9-f2342dc66f5e.png)
      - 索引分析器将值转换为tokens，并标准化。如下图：每个token是一个值，每个token都被索引
        ![image](https://user-images.githubusercontent.com/46510621/132093720-e429a960-cf2d-45f9-82ab-9be69aa5a152.png)
      - 当搜索"Quick fox"时，查询语句会被分析器转换为[quick, fox]，因此搜索命中
