# 数据读取与保存

  - Spark的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。
    - 文件格式分为：Text文件、Json文件、Csv文件、Sequence文件以及Object文件。
    - 文件系统分为：本地文件系统、HDFS、HBase、Hive、Elasticsearch以及数据库。
  
## 文件类数据读取与保存

  - Text文件：
    - 数据读取: sc.textFile(String)
    - 数据保存: saveAsTextFile(String)
  - Json文件：
    - 如果JSON文件中每一行就是一个JSON记录，那么可以通过将JSON文件当做文本文件来读取，然后利用相关的JSON库对每一条数据进行JSON解析。
    - SparkSQL集成了很好的处理JSON文件的方式，所以应用中多是采用SparkSQL处理JSON文件。
  - Sequence文件：
    - SequenceFile文件是Hadoop用来存储二进制形式的键值对数据的文件格式。
    - Spark有专门用来读取/保存SequenceFile的接口：sc.sequenceFile(path)和saveAsSequenceFile()
    - 注意：SequenceFile文件只针对PairRDD。
  - 对象文件：
    - 对象文件是将对象序列化后保存的文件，采用Java的序列化机制。
    - 可以通过objectFile(path) 函数接收一个路径，读取对象文件，返回对应的RDD，也可以通过调用saveAsObjectFile()实现对对象文件的输出。
    
## 文件系统类数据读取与保存

  - HDFS：
    - Spark为了能够兼容Hadoop所有的版本,对于外部存储创建操作而言,使用hadoopFile（旧API）和newAPIHadoopFile两个函数接口，主要包含以下四个参数：
      - 输入格式(InputFormat)：制定数据输入的类型
      - 键类型: 指定[K,V]键值对中K的类型
      - 值类型: 指定[K,V]键值对中V的类型
      - 额外的Hadoop配置属性通过conf对象传入
      
## Hive

  - 使用Spark SQL连接到Hive，需要将配置文件hive-site.xml，core-site.xml，hdfs-site.xml复制到Spark的./conf/目录下。
  - 设置spark.sql.warehouse.dir指定数据库的默认位置
  - 代码示例：
    ```
    val spark = SparkSession.builder().appName("Hive Example")
                    .config("spark.sql.warehouse.dir", warehouseLocation).enableHiveSupport().getOrCreate()
    
    import spark.implicits._
    import spark.sql
    
    sql("SELECT * FROM cnTable").show()
    ```
  
## HBase

  - 补充新API
  
