# 数据读取与保存

## 动机

  - 三类常见的数据源：
    - 文件格式与文件系统：
      - Spark 可以访问很多种不同的文件格式，包括文本文件、JSON、SequenceFile，以及protocol buffer。
    - Spark SQL中的结构化数据源
    - 数据库与键值存储：
      - Spark 自带的库和一些第三方库，它们可以用来连接 Cassandra、HBase、Elasticsearch 以及 JDBC 源。
  
## 文件格式

  - Spark支持的一些常见格式：
  
    ![Spark支持的一些常见格式](./图片/Spark支持的一些常见格式.PNG)
    
  - 文本文件：
    - 将一个文本文件读取为RDD时，输入的每一行都会成为RDD的一个元素。也可以将多个完整的文本文件一次性读取为一个pair RDD，其中键是文件名，值是文件内容。
    - 读取文本文件：
      - textFile()：读取一个文本文件。
      - wholeTextFiles()：处理多个输入文件，该方法会返回一个 pair RDD，其中键是输入文件的文件名。
    - 保存文本文件：
      - saveAsTextFile()：接收一个路径，并将RDD 中的内容都输入到路径对应的文件中。Spark 将传入的路径作为目录对待，会在那个目录下输出多个文件。
  - JSON：
    - 读取JSON：
      - 将数据作为文本文件读取，然后对 JSON 数据进行解析。这种方法假设文件中的每一行都是一条 JSON 记录。如果你有跨行的JSON 数据，你就只能读入整个文件，然后对每个文件进行解析。
    - 保存JSON：
      - 将由结构化数据组成的RDD转为字符串RDD，然后使用 Spark 的文本文件 API 写出去。
      - 例子：在 Scala 中保存为 JSON
        ```
        result.filter(p => P.lovesPandas).map(mapper.writeValueAsString(_)).saveAsTextFile(outputFile)
        ```
  - 逗号分隔值与制表符分隔值：
    - 逗号分隔值（CSV）文件每行都有固定数目的字段，字段间用逗号隔开（在制表符分隔值文件，即 TSV 文件中用制表符隔开）。
    - 读取CSV：
      - 对于Python我们会使用自带的csv库，在Scala和Java中则使用opencsv库。
      - 如果CSV的所有数据字段均没有包含换行符，你也可以使用 textFile() 读取并解析数据。如果在字段中嵌有换行符，就需要完整读入每个文件，然后解析各段。
    - 保存CSV：
      - 使用的CSV库要输出到文件或者输出器，可以使用StringWriter或StringIO来将结果放到RDD中。
  - SequenceFile：
    - SequenceFile是由没有相对关系结构的键值对文件组成的常用 Hadoop 格式。
    - SequenceFile文件有同步标记，Spark可以用它来定位到文件中的某个点，然后再与记录的边界对齐。
    - 读取SequenceFile：
      - 在SparkContext中，可以调用sequenceFile(path, keyClass, valueClass, minPartitions)。
      - SequenceFile使用Writable类，因此keyClass和valueClass参数都必须使用正确的Writable类。
      - Hadoop Writable类型
        
        ![Writable类型](./图片/Writable类型.PNG)
    
      
